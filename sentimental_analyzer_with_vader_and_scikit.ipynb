{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chitrankdixit/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/chitrankdixit/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "\n",
    "# for named entity\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, tree, download, tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# for preparing data\n",
    "download('vader_lexicon')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# nltk for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'./playstorereviewsdata/' # use your path\n",
    "allFiles = glob.glob(path + \"/*.tsv\")\n",
    "\n",
    "review_data = [];\n",
    "count = 0\n",
    "for out_count, file in enumerate(allFiles):\n",
    "    with open(file, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for counter, row in enumerate(spamreader):\n",
    "            #import pdb;pdb.set_trace()\n",
    "            count += 1\n",
    "            if counter == 0:\n",
    "                text_index = row.index(\"Review Text\")\n",
    "                title_index = row.index(\"Review Title\")\n",
    "                rating_index = row.index(\"Star Rating\")\n",
    "                #print(text_index, title_index, rating_index)\n",
    "            else:\n",
    "                for item in row[text_index].split('.'):\n",
    "                    if not re.findall(r'[\\w.-]+@[\\w.-]+',item):\n",
    "                        review_data.append({\n",
    "                            \"Review Text\": item,\n",
    "                            \"Review Title\": row[title_index],\n",
    "                            \"Star Rating\": int(row[rating_index])\n",
    "                        })\n",
    "            \n",
    "#     with open(file+out_count, 'wt') as csvfile:\n",
    "#             writer = csv.writer(csvfile, delimiter='', quotechar='|')\n",
    "#             #writer.writerow([\"#\"] + anarkali_characteristics)\n",
    "#             #import pdb;pdb.set_trace()\n",
    "#             for element in lehenga_characteristics:\n",
    "#                 #import pdb;pdb.set_trace()\n",
    "#                 writer.writerow([element[\"name\"], element[\"count\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review Text     22790\n",
       "Review Title    22790\n",
       "Star Rating     22790\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get full data\n",
    "\n",
    "\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "\n",
    "# datacolumns = [\n",
    "#     \"Package Name\",\n",
    "#     \"App Version Code\",\n",
    "#     \"Reviewer Language\",\n",
    "#     \"Device\",\n",
    "#     \"Review Submit Date and Time\",\n",
    "#     \"Review Submit Millis Since Epoch\",\n",
    "#     \"Review Last Update Date and Time\",\n",
    "#     \"Review Last Update Millis Since Epoch\",\n",
    "#     \"Star Rating\",\n",
    "#     \"Review Title\",\n",
    "#     \"Review Text\",\n",
    "#     \"Developer Reply Date and Time\",\n",
    "#     \"Developer Reply Millis Since Epoch\",\n",
    "#     \"Developer Reply Text,Review Link\"\n",
    "# ]\n",
    "\n",
    "# for item in allFiles:\n",
    "    \n",
    "\n",
    "# for file_ in allFiles:\n",
    "#     df = pd.read_csv(file_,index_col=None, header=0, encoding = \"ISO-8859-1\", error_bad_lines=False, sep='\\t')\n",
    "#     list_.append(df)\n",
    "    \n",
    "    \n",
    "# full_data = pd.concat(list_)\n",
    "\n",
    "#import pdb;pdb.set_trace()\n",
    "full_data = pd.DataFrame(review_data)\n",
    "full_data.count()\n",
    "\n",
    "# import pdb;pdb.set_trace()\n",
    "# full_data1 = pd.concat(list_)\n",
    "# full_data1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the NaN \n",
    "full_data[\"Review Text\"] = full_data[\"Review Text\"].fillna('')\n",
    "full_data[\"Review Title\"] = full_data[\"Review Title\"].fillna('')\n",
    "# full_data[\"App Version Code\"] = full_data[\"App Version Code\"].fillna(0.0)\n",
    "# full_data[\"App Version Name\"] = full_data[\"App Version Name\"].fillna('')\n",
    "# full_data[\"Developer Reply Date and Time\"] = full_data[\"Developer Reply Date and Time\"].fillna('')\n",
    "# full_data[\"Developer Reply Millis Since Epoch\"] = full_data[\"Developer Reply Millis Since Epoch\"].fillna('')\n",
    "# full_data[\"Developer Reply Text\"] = full_data[\"Developer Reply Text\"].fillna('')\n",
    "\n",
    "# slicing the required columns only\n",
    "#full_data = full_data[[\"Review Text\", \"Review Title\", \"Star Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Star Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a brilliant concept</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please start for men's asap</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brilliant stuff!! Waiting to see more</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Review Text    Review Title  Star Rating\n",
       "0            This is a brilliant concept  Great product!            5\n",
       "1            Please start for men's asap  Great product!            5\n",
       "2                   Brilliant concept :)                            5\n",
       "3                                                                   5\n",
       "4  Brilliant stuff!! Waiting to see more                            5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.loc[full_data['Review Text'] != '']\n",
    "\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#import pdb;pdb.set_trace()\n",
    "\n",
    "\n",
    "# print(full_data['Review Text'].tolist()[0])\n",
    "# ss = sid.polarity_scores(full_data['Review Text'].tolist()[0])\n",
    "# print(ss)\n",
    "\n",
    "\n",
    "# add a named entities to look for\n",
    "#import pdb;pdb.set_trace()\n",
    "\n",
    "def get_sentiment_score(item):\n",
    "    sentences = item['Review Text']\n",
    "    \n",
    "    ss = sid.polarity_scores(sentences)\n",
    "    negative = ss[\"neg\"]\n",
    "    positive = ss[\"pos\"]\n",
    "    neutral = ss[\"neu\"]\n",
    "    #compound = ss[\"compound\"]\n",
    "    return pd.Series({'negative': negative, 'positive': positive, 'neutral': neutral})\n",
    "\n",
    "\n",
    "scores = full_data.apply(get_sentiment_score, axis=1)\n",
    "\n",
    "final_sliced_data = pd.concat([ full_data , scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a brilliant concept</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please start for men's asap</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brilliant stuff!! Waiting to see more</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This idea will be considered as one of the top...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text    Review Title  \\\n",
       "0                        This is a brilliant concept  Great product!   \n",
       "1                        Please start for men's asap  Great product!   \n",
       "2                               Brilliant concept :)                   \n",
       "4              Brilliant stuff!! Waiting to see more                   \n",
       "8  This idea will be considered as one of the top...       Excellent   \n",
       "\n",
       "   Star Rating  negative  neutral  positive  \n",
       "0            5       0.0    0.441     0.559  \n",
       "1            5       0.0    0.635     0.365  \n",
       "2            5       0.0    0.128     0.872  \n",
       "4            5       0.0    0.533     0.467  \n",
       "8            5       0.0    0.886     0.114  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sliced_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate csv file\n",
    "final_sliced_data.to_csv(\"review_text_data.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114e4ae48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHYaDJKnDcJAkdfw/Y95+gIXAu6IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11264a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_sliced_data['Star Rating'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sliced_data.to_csv('pos_neg_score.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13477\n",
      "3970\n",
      "546\n"
     ]
    }
   ],
   "source": [
    "# 0 -> neutral\n",
    "# 1 -> negative\n",
    "# 2 -> positive\n",
    "# 3 -> compound\n",
    "print(final_sliced_data.loc[final_sliced_data['negative'] == 0.0].__len__())\n",
    "print(final_sliced_data.loc[final_sliced_data['positive'] == 0.0].__len__())\n",
    "print(final_sliced_data.loc[final_sliced_data['neutral'] == 0.0].__len__())\n",
    "#print(final_sliced_data.loc[final_sliced_data['compound'] == 0.0].__len__())\n",
    "\n",
    "def get_sentiment_class(item):\n",
    "    sentiment_class = 0\n",
    "    score_list = [item['neutral'], item['negative'], item['positive']]\n",
    "    max_val = max(score_list)\n",
    "    \n",
    "    sentiment_class = score_list.index(max_val)\n",
    "    \n",
    "    return pd.Series({'sentiment_class': sentiment_class})\n",
    "\n",
    "\n",
    "score_class = final_sliced_data.apply(get_sentiment_class, axis=1)\n",
    "\n",
    "input_data = pd.concat([ final_sliced_data , score_class], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13207\n",
      "158\n",
      "2296\n"
     ]
    }
   ],
   "source": [
    "print(input_data.loc[input_data['sentiment_class'] == 0].__len__())\n",
    "print(input_data.loc[input_data['sentiment_class'] == 1].__len__())\n",
    "print(input_data.loc[input_data['sentiment_class'] == 2].__len__())\n",
    "#print(input_data.loc[input_data['sentiment_class'] == 3].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_data = input_data.loc[(input_data['sentiment_class'] == 0)][:158]\n",
    "negative_data = input_data.loc[(input_data['sentiment_class'] == 1)][:158]\n",
    "positive_data = input_data.loc[(input_data['sentiment_class'] == 2)][:158]\n",
    "#compound_data = input_data.loc[(input_data['sentiment_class'] == 3)][:158]\n",
    "# add a named entities to look for\n",
    "#import pdb;pdb.set_trace()\n",
    "\n",
    "intermediate_data1 = positive_data.append(negative_data)\n",
    "sliced_data = intermediate_data1.append(neutral_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "158\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "print(sliced_data.loc[sliced_data['sentiment_class'] == 0].__len__())\n",
    "print(sliced_data.loc[sliced_data['sentiment_class'] == 1].__len__())\n",
    "print(sliced_data.loc[sliced_data['sentiment_class'] == 2].__len__())\n",
    "#print(sliced_data.loc[sliced_data['sentiment_class'] == 3].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>Came across this app through fb promotions</td>\n",
       "      <td>Beautiful app, necessary to experience</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>Was surprised to think why nobody ever did th...</td>\n",
       "      <td>Beautiful app, necessary to experience</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Anyway, like the concept, affordable pricing ...</td>\n",
       "      <td>Beautiful app, necessary to experience</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>The pictures just don't load! I have a 4g conn...</td>\n",
       "      <td>Very slow</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>I dont have fb account</td>\n",
       "      <td>login without fb account??</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Review Text  \\\n",
       "528         Came across this app through fb promotions   \n",
       "529   Was surprised to think why nobody ever did th...   \n",
       "530   Anyway, like the concept, affordable pricing ...   \n",
       "538  The pictures just don't load! I have a 4g conn...   \n",
       "541                             I dont have fb account   \n",
       "\n",
       "                               Review Title  Star Rating  negative  neutral  \\\n",
       "528  Beautiful app, necessary to experience            5       0.0    1.000   \n",
       "529  Beautiful app, necessary to experience            5       0.0    0.705   \n",
       "530  Beautiful app, necessary to experience            5       0.0    0.737   \n",
       "538                               Very slow            2       0.0    0.870   \n",
       "541              login without fb account??            2       0.0    1.000   \n",
       "\n",
       "     positive  sentiment_class  \n",
       "528     0.000                0  \n",
       "529     0.295                0  \n",
       "530     0.263                0  \n",
       "538     0.130                0  \n",
       "541     0.000                0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaderVectorizer:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract_classes(self):\n",
    "        pass\n",
    "    \n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in word_tokenize(sent)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0d79b847569f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#model2.fit(train_corpus_tf_idf,y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#model3.fit(train_corpus_tf_idf,y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#result1 = model1.predict(test_corpus_tf_idf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "labels = sliced_data[\"sentiment_class\"].values;\n",
    "\n",
    "#import pdb;pdb.set_trace()  \n",
    "kf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "\n",
    "totalNBV = 0\n",
    "\n",
    "totalMatMLP = np.zeros((3,3));\n",
    "\n",
    "# tr, te = kf.split(sliced_data['Review Text'], labels)\n",
    "# td = []\n",
    "# for i in train_index:\n",
    "#     td.append(sliced_data['Review Text'][i])\n",
    "#import pdb;pdb.set_trace()\n",
    "review_text = sliced_data['Review Text'].tolist()\n",
    "\n",
    "# break words\n",
    "review_words = []\n",
    "\n",
    "# for item in review_text:\n",
    "#     review_words += item.split(\" \")\n",
    "counter = 0\n",
    "for train_index, test_index in kf.split(review_text, labels):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(train_index.__len__(), test_index.__len__())\n",
    "    #X_train = [review_text[i] for i in train_index]\n",
    "    #X_test = [review_text[i] for i in test_index]\n",
    "    #import pdb;pdb.set_trace()\n",
    "    X_train_nb = [[format_sentence(review_text[i]), int(sliced_data.iloc[i]['sentiment_class'])] for i in train_index]\n",
    "    X_test_nb = [[format_sentence(review_text[i]), int(sliced_data.iloc[i]['sentiment_class'])] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "#     vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=False, stop_words='english')\n",
    "#     train_corpus_tf_idf = vectorizer.fit_transform(X_train) \n",
    "#     test_corpus_tf_idf = vectorizer.transform(X_test)\n",
    "    \n",
    "    \n",
    "    #vader_vectorizer = \n",
    "    #import pdb;pdb.set_trace()\n",
    "    #import pdb;pdb.set_trace()\n",
    "#     splitted_x_train = [item.split(' ') for item in X_train]\n",
    "#     w = WordWeightAssign(splitted_x_train)\n",
    "#     w.collect_data()\n",
    "#     w.score_words()\n",
    "#     train_corpus_scores = w.score_words()\n",
    "    \n",
    "#     splitted_x_test = [item.split(' ') for item in X_test]\n",
    "#     w = WordWeightAssign(splitted_x_test)\n",
    "#     w.collect_data()\n",
    "#     w.score_words()\n",
    "#     test_corpus_scores = w.score_words()\n",
    "    #model1 = LinearSVC()\n",
    "    #model2 = MultinomialNB()\n",
    "    #model3 = MLPClassifier()\n",
    "    nb = MultinomialNB()\n",
    "    # Max entropy\n",
    "    #model1.fit(train_corpus_tf_idf,y_train)\n",
    "    #model2.fit(train_corpus_tf_idf,y_train)\n",
    "    #model3.fit(train_corpus_tf_idf,y_train)\n",
    "    nb.fit(X_train_nb, y_train)\n",
    "    \n",
    "    #result1 = model1.predict(test_corpus_tf_idf)\n",
    "    #result2 = model2.predict(test_corpus_tf_idf)\n",
    "    #result3 = model3.predict(test_corpus_tf_idf)\n",
    "    result4 = nb.predict(X_test_nb)\n",
    "    \n",
    "    #test1 = model1.score(test_corpus_tf_idf, y_test)\n",
    "    #test2 = model2.score(test_corpus_tf_idf, y_test)\n",
    "    #test3 = model3.score(test_corpus_tf_idf, y_test)\n",
    "    test4 = nb.score(X_test_nb, y_test)\n",
    "    print(test4)\n",
    "    \n",
    "    #totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    #totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatNBV = totalMatNBV + confusion_matrix(y_test, result4)\n",
    "    #totalMatMLP = totalMatMLP + confusion_matrix(y_test, result3)\n",
    "    #totalsvm = totalsvm + sum(y_test==result1)\n",
    "    #totalNB = totalNB + sum(y_test==result2)\n",
    "    totalNBV = totalNBV + sum(y_test==result4)\n",
    "    #totalMLP = totalMLP + sum(y_test==result3)\n",
    "    counter += 1\n",
    "    test_data = [\"I do not liked the service.\", \"What is the point of starting this when you can not deliver items on time\", \"Is it really working in India ?\"]\n",
    "#     splitted_x_test_ = [item.split(' ') for item in test_data]\n",
    "#     w = WordWeightAssign(splitted_x_test_)\n",
    "#     w.collect_data()\n",
    "#     test_corpus = w.score_words()\n",
    "    \n",
    "#     result_test = nb.predict(test_corpus)\n",
    "#     print(result_test)\n",
    "    prev = sliced_data.iloc[test_index, :]\n",
    "    test = pd.DataFrame( { 'Review Text': X_test , 'Sentiment': result4,\n",
    "                          'Neutral': prev['neutral'],\n",
    "                          'Positive': prev[\"positive\"], \n",
    "                          'Negative': prev[\"negative\"],\n",
    "                         'Star Rating': prev[\"Star Rating\"]} )\n",
    "    test.to_csv( 'sentiment_prediction'+str(counter)+'.csv' , index = False )\n",
    "    \n",
    "    \n",
    "print((totalNBV/474.0) * 100.00)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(totalMatNBV, classes=[\"neutral\", \"negative\", \"positive\"], normalize=True,\n",
    "                      title='confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "#print(totalMatSvm, totalsvm, totalMatNB, totalNB, totalMatMLP, totalMLP)\n",
    "#test.to_csv( 'sentiment_prediction.csv' , index = False )\n",
    "\n",
    "#print(train_corpus_tf_idf, test_corpus_tf_idf)\n",
    "#import pdb;pdb.set_trace()\n",
    "#print(test_corpus_tf_idf, X_test[0])\n",
    "\n",
    "#test_Y = model2.predict( test_corpus_tf_idf )\n",
    "#print(test_Y)\n",
    "#print(X_test)\n",
    "\n",
    "#print(results_list)\n",
    "#print(review_text.reset_index(drop=True))\n",
    "#print(review_text)\n",
    "# test = pd.DataFrame( { 'Review Text': review_text , 'Sentiment': test_Y } )\n",
    "# test.shape\n",
    "# test.head()\n",
    "#test.to_csv( 'sentiment_prediction.csv' , index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
