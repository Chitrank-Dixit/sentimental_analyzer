{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/chitrankdixit/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import csv\n",
    "import sys,os\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "\n",
    "# for named entity\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, tree, download\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# for preparing data\n",
    "download('vader_lexicon')\n",
    "%matplotlib inline\n",
    "\n",
    "#!export GOOGLE_APPLICATION_CREDENTIALS='sentimental.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='sentimental.json'\n",
    "\n",
    "# nltk for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'./playstore_corpus_tsv/' # use your path\n",
    "allFiles = glob.glob(path + \"/*.tsv\")\n",
    "\n",
    "# for out_count, file in enumerate(allFiles):\n",
    "#     with open(file, newline='') as csvfile:\n",
    "#         spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         for counter, row in enumerate(spamreader):\n",
    "#             import pdb;pdb.set_trace()\n",
    "#             print(row)\n",
    "            \n",
    "#     with open(file+out_count, 'wt') as csvfile:\n",
    "#             writer = csv.writer(csvfile, delimiter='', quotechar='|')\n",
    "#             #writer.writerow([\"#\"] + anarkali_characteristics)\n",
    "#             #import pdb;pdb.set_trace()\n",
    "#             for element in lehenga_characteristics:\n",
    "#                 #import pdb;pdb.set_trace()\n",
    "#                 writer.writerow([element[\"name\"], element[\"count\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Subject</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>2</td>\n",
       "      <td>[\" Please start for men's asap\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brilliant concept :)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description         Subject  \\\n",
       "0  This is a brilliant concept. Please start for ...  Great product!   \n",
       "1                               Brilliant concept :)             NaN   \n",
       "2                                                NaN             NaN   \n",
       "3            Brilliant stuff!! Waiting to see more..             NaN   \n",
       "4                                                NaN             NaN   \n",
       "\n",
       "   sentiment                               tag  \n",
       "0          2  [\" Please start for men's asap\"]  \n",
       "1          2          ['Brilliant concept :)']  \n",
       "2         -1                                []  \n",
       "3          1                              ['']  \n",
       "4         -1                                []  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get full data\n",
    "\n",
    "\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "\n",
    "# datacolumns = [\n",
    "#     \"Package Name\",\n",
    "#     \"App Version Code\",\n",
    "#     \"Reviewer Language\",\n",
    "#     \"Device\",\n",
    "#     \"Review Submit Date and Time\",\n",
    "#     \"Review Submit Millis Since Epoch\",\n",
    "#     \"Review Last Update Date and Time\",\n",
    "#     \"Review Last Update Millis Since Epoch\",\n",
    "#     \"Star Rating\",\n",
    "#     \"Review Title\",\n",
    "#     \"Review Text\",\n",
    "#     \"Developer Reply Date and Time\",\n",
    "#     \"Developer Reply Millis Since Epoch\",\n",
    "#     \"Developer Reply Text,Review Link\"\n",
    "# ]\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, encoding = \"ISO-8859-1\", error_bad_lines=False, sep='\\t')\n",
    "    list_.append(df)\n",
    "    \n",
    "    \n",
    "full_data = pd.concat(list_)\n",
    "full_data.head(5)\n",
    "\n",
    "#mail_data = pd.read_csv(\"parsed_data.tsv\",index_col=None, header=0, encoding = \"ISO-8859-1\", error_bad_lines=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "> <ipython-input-16-e08393cc6d35>(1)<module>()->None\n",
      "-> import pdb;pdb.set_trace()\n",
      "(Pdb) full_data\n",
      "                                           Description  \\\n",
      "0    This is a brilliant concept. Please start for ...   \n",
      "1                                 Brilliant concept :)   \n",
      "2                                                  NaN   \n",
      "3              Brilliant stuff!! Waiting to see more..   \n",
      "4                                                  NaN   \n",
      "5    This idea will be considered as one of the top...   \n",
      "6                                                  NaN   \n",
      "7    No spending on buying apparels for gf... Why t...   \n",
      "8    Brilliant idea!! Adorably splendid dresses at ...   \n",
      "9                                                  NaN   \n",
      "10                                                 NaN   \n",
      "11                                                 NaN   \n",
      "12                                                 NaN   \n",
      "13   The whole concept looks great. Something to re...   \n",
      "14                                                 NaN   \n",
      "15                                                 NaN   \n",
      "16   Nice UI! Loved the clothes. When are you start...   \n",
      "17                                                 NaN   \n",
      "18                                                 NaN   \n",
      "19                                                 NaN   \n",
      "20                                                 NaN   \n",
      "21                                                 NaN   \n",
      "22   Reserving a spot on my phone's home screen for...   \n",
      "23                                                 NaN   \n",
      "24                                                 NaN   \n",
      "25                    Waiting for you guys to go live!   \n",
      "26                                                 NaN   \n",
      "27   This is gonna be one of the best in the market...   \n",
      "28                            Can't wait to see more!!   \n",
      "29                                                 NaN   \n",
      "..                                                 ...   \n",
      "970  assist me to save a number of cash I spend qui...   \n",
      "971  Renting garments is a excellent concept it hel...   \n",
      "972  were using flyrobe for a bypass now and absolu...   \n",
      "973  totally well worth the download this utility a...   \n",
      "974  Roposo unique like programs are not correct th...   \n",
      "975                             Ok We have nice things   \n",
      "976  Very unique idea i've been the use of this app...   \n",
      "977  i exploit this app first rate application plea...   \n",
      "978  surely one of the mousetrap you'll be amazed t...   \n",
      "979  Very cool caption approximately statistics abo...   \n",
      "980  wedding dresses of birthday celebration dresse...   \n",
      "981  style fashion designer in which may be to be h...   \n",
      "982  Flyrobe has masses of blessings over different...   \n",
      "983  first-rate best first-class software preserve ...   \n",
      "984  one of the great easy and protection shield se...   \n",
      "985  No compromise on do they have got incredible a...   \n",
      "986  Flyrobe has so many advantages that you will b...   \n",
      "987  unfastened apartment services for fashion desi...   \n",
      "988                                                NaN   \n",
      "989  koovs and roposo like utility or now not well ...   \n",
      "990  were using flyrobe for quite a long term now v...   \n",
      "991  For each form of event you may hire clothes ev...   \n",
      "992  while an purchasing is made so much less compl...   \n",
      "993  Tremendous application and terrific offerings ...   \n",
      "994  the fine manner to hire cloths is that this. S...   \n",
      "995  Wow! Never knew that it might be this smooth t...   \n",
      "996  A huge thank you for making it so easy and fun...   \n",
      "997                                It is very good app   \n",
      "998  It is so higher to lease cloths! Facilitates m...   \n",
      "999  It's miles higher manner to lease cloths. I am...   \n",
      "\n",
      "                                  Subject  sentiment  \\\n",
      "0                          Great product!          2   \n",
      "1                                     NaN          2   \n",
      "2                                     NaN         -1   \n",
      "3                                     NaN          1   \n",
      "4                                     NaN         -1   \n",
      "5                               Excellent          2   \n",
      "6                                     NaN         -1   \n",
      "7    Brilliant ..this is what we needed!!          1   \n",
      "8                                     NaN          2   \n",
      "9                                     NaN         -1   \n",
      "10                                    NaN         -1   \n",
      "11                                    NaN         -1   \n",
      "12                                    NaN         -1   \n",
      "13                           Awesome app.          2   \n",
      "14                                    NaN         -1   \n",
      "15                                    NaN         -1   \n",
      "16         Looks like tinder for clothes!          2   \n",
      "17                                    NaN         -1   \n",
      "18                                    NaN         -1   \n",
      "19                                    NaN         -1   \n",
      "20                                    NaN         -1   \n",
      "21                                    NaN         -1   \n",
      "22                                    NaN          1   \n",
      "23                                    NaN         -1   \n",
      "24                                    NaN         -1   \n",
      "25                 Sounds really exciting          1   \n",
      "26                                    NaN         -1   \n",
      "27                 This app is gonna rock          2   \n",
      "28                        Awesome stuff!!          2   \n",
      "29                                    NaN         -1   \n",
      "..                                    ...        ...   \n",
      "970                                   NaN          1   \n",
      "971                                   NaN          2   \n",
      "972                                   NaN          2   \n",
      "973                                   NaN          2   \n",
      "974                                   NaN          1   \n",
      "975                                   NaN          2   \n",
      "976                                   NaN          2   \n",
      "977                                   NaN          1   \n",
      "978                                   NaN          2   \n",
      "979                                   NaN          2   \n",
      "980                                   NaN          2   \n",
      "981                                   NaN          2   \n",
      "982                                   NaN          2   \n",
      "983                                   NaN          2   \n",
      "984                                   NaN          2   \n",
      "985                                   NaN          2   \n",
      "986                                   NaN          2   \n",
      "987                                   NaN          2   \n",
      "988                                   NaN         -1   \n",
      "989                                   NaN          2   \n",
      "990                                   NaN          2   \n",
      "991                                   NaN          2   \n",
      "992                                   NaN          2   \n",
      "993                                   NaN          2   \n",
      "994                                   NaN          2   \n",
      "995                                   NaN          2   \n",
      "996                                   NaN          2   \n",
      "997                                   NaN          2   \n",
      "998                                   NaN          1   \n",
      "999                                   NaN          2   \n",
      "\n",
      "                                                   tag  \n",
      "0                     [\" Please start for men's asap\"]  \n",
      "1                             ['Brilliant concept :)']  \n",
      "2                                                   []  \n",
      "3                                                 ['']  \n",
      "4                                                   []  \n",
      "5                                                 ['']  \n",
      "6                                                   []  \n",
      "7                     [' Why to buy if u can rent it']  \n",
      "8                                     ['', 'jwellery']  \n",
      "9                                                   []  \n",
      "10                                                  []  \n",
      "11                                                  []  \n",
      "12                                                  []  \n",
      "13                                                ['']  \n",
      "14                                                  []  \n",
      "15                                                  []  \n",
      "16                 [\" When are you starting men's??!\"]  \n",
      "17                                                  []  \n",
      "18                                                  []  \n",
      "19                                                  []  \n",
      "20                                                  []  \n",
      "21                                                  []  \n",
      "22   [\"Reserving a spot on my phone's home screen f...  \n",
      "23                                                  []  \n",
      "24                                                  []  \n",
      "25                ['Waiting for you guys to go live!']  \n",
      "26                                                  []  \n",
      "27                                          ['', ' !']  \n",
      "28                        [\"Can't wait to see more!!\"]  \n",
      "29                                                  []  \n",
      "..                                                 ...  \n",
      "970  ['assist me to save a number of cash I spend q...  \n",
      "971  ['Renting garments is a excellent concept it h...  \n",
      "972  ['were using flyrobe for a bypass now and abso...  \n",
      "973  [\"totally well worth the download this utility...  \n",
      "974  [\"Roposo unique like programs are not correct ...  \n",
      "975                         ['Ok We have nice things']  \n",
      "976  [\"Very unique idea i've been the use of this a...  \n",
      "977  ['i exploit this app first rate application pl...  \n",
      "978  [\"surely one of the mousetrap you'll be amazed...  \n",
      "979  [\"Very cool caption approximately statistics a...  \n",
      "980  ['wedding dresses of birthday celebration dres...  \n",
      "981  ['style fashion designer in which may be to be...  \n",
      "982  ['Flyrobe has masses of blessings over differe...  \n",
      "983  ['first-rate best first-class software preserv...  \n",
      "984  ['one of the great easy and protection shield ...  \n",
      "985  ['No compromise on do they have got incredible...  \n",
      "986  ['Flyrobe has so many advantages that you will...  \n",
      "987  ['unfastened apartment services for fashion de...  \n",
      "988                                                 []  \n",
      "989  ['koovs and roposo like utility or now not wel...  \n",
      "990  [\"were using flyrobe for quite a long term now...  \n",
      "991  [\"For each form of event you may hire clothes ...  \n",
      "992                                               ['']  \n",
      "993  [\"Tremendous application and terrific offering...  \n",
      "994                                               ['']  \n",
      "995                                               ['']  \n",
      "996                                               ['']  \n",
      "997                            ['It is very good app']  \n",
      "998                    [' So higher and high-quality']  \n",
      "999       [\"It's miles higher manner to lease cloths\"]  \n",
      "\n",
      "[10683 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) full_data['sentiment'][0]\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "0    2\n",
      "Name: sentiment, dtype: int64\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Description',\n",
       " 'Subject',\n",
       " 'T',\n",
       " '_AXIS_ALIASES',\n",
       " '_AXIS_IALIASES',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_NAMES',\n",
       " '_AXIS_NUMBERS',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_REVERSED',\n",
       " '_AXIS_SLICEMAP',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_add_numeric_operations',\n",
       " '_add_series_only_operations',\n",
       " '_add_series_or_dataframe_operations',\n",
       " '_agg_by_level',\n",
       " '_agg_doc',\n",
       " '_aggregate',\n",
       " '_aggregate_multiple_funcs',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_apply_broadcast',\n",
       " '_apply_empty_result',\n",
       " '_apply_raw',\n",
       " '_apply_standard',\n",
       " '_at',\n",
       " '_box_col_values',\n",
       " '_box_item_values',\n",
       " '_builtin_table',\n",
       " '_check_inplace_setting',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_percentile',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_combine_const',\n",
       " '_combine_frame',\n",
       " '_combine_match_columns',\n",
       " '_combine_match_index',\n",
       " '_combine_series',\n",
       " '_combine_series_infer',\n",
       " '_compare_frame',\n",
       " '_compare_frame_evaluate',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_axes_dict_for_slice',\n",
       " '_construct_axes_dict_from',\n",
       " '_construct_axes_from_arguments',\n",
       " '_constructor',\n",
       " '_constructor_expanddim',\n",
       " '_constructor_sliced',\n",
       " '_convert',\n",
       " '_count_level',\n",
       " '_create_indexer',\n",
       " '_cython_table',\n",
       " '_deprecations',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_drop_axis',\n",
       " '_ensure_valid_index',\n",
       " '_expand_axes',\n",
       " '_flex_compare_frame',\n",
       " '_from_arrays',\n",
       " '_from_axes',\n",
       " '_get_agg_axis',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cacher',\n",
       " '_get_index_resolvers',\n",
       " '_get_item_cache',\n",
       " '_get_numeric_data',\n",
       " '_get_valid_indices',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_getitem_array',\n",
       " '_getitem_column',\n",
       " '_getitem_frame',\n",
       " '_getitem_multilevel',\n",
       " '_getitem_slice',\n",
       " '_gotitem',\n",
       " '_iat',\n",
       " '_iget_item_cache',\n",
       " '_iloc',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_info_repr',\n",
       " '_init_dict',\n",
       " '_init_mgr',\n",
       " '_init_ndarray',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_builtin_func',\n",
       " '_is_cached',\n",
       " '_is_cython_func',\n",
       " '_is_datelike_mixed_type',\n",
       " '_is_mixed_type',\n",
       " '_is_numeric_mixed_type',\n",
       " '_is_view',\n",
       " '_ix',\n",
       " '_ixs',\n",
       " '_join_compat',\n",
       " '_loc',\n",
       " '_maybe_cache_changed',\n",
       " '_maybe_update_cacher',\n",
       " '_metadata',\n",
       " '_needs_reindex_multi',\n",
       " '_obj_with_exclusions',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reindex_axes',\n",
       " '_reindex_axis',\n",
       " '_reindex_columns',\n",
       " '_reindex_index',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_repr_data_resource_',\n",
       " '_repr_fits_horizontal_',\n",
       " '_repr_fits_vertical_',\n",
       " '_repr_html_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_sanitize_column',\n",
       " '_selected_obj',\n",
       " '_selection',\n",
       " '_selection_list',\n",
       " '_selection_name',\n",
       " '_series',\n",
       " '_set_as_cached',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_is_copy',\n",
       " '_set_item',\n",
       " '_set_value',\n",
       " '_setitem_array',\n",
       " '_setitem_frame',\n",
       " '_setitem_slice',\n",
       " '_setup_axes',\n",
       " '_shallow_copy',\n",
       " '_slice',\n",
       " '_stat_axis',\n",
       " '_stat_axis_name',\n",
       " '_stat_axis_number',\n",
       " '_take',\n",
       " '_to_dict_of_blocks',\n",
       " '_try_aggregate_string_function',\n",
       " '_typ',\n",
       " '_unpickle_frame_compat',\n",
       " '_unpickle_matrix_compat',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " '_xs',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'applymap',\n",
       " 'as_matrix',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'assign',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'axes',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'boxplot',\n",
       " 'clip',\n",
       " 'clip_lower',\n",
       " 'clip_upper',\n",
       " 'columns',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compound',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'corrwith',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'eval',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'floordiv',\n",
       " 'from_dict',\n",
       " 'from_items',\n",
       " 'from_records',\n",
       " 'ftypes',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'get_dtype_counts',\n",
       " 'get_ftype_counts',\n",
       " 'get_values',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'info',\n",
       " 'insert',\n",
       " 'interpolate',\n",
       " 'is_copy',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'iterrows',\n",
       " 'itertuples',\n",
       " 'ix',\n",
       " 'join',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lookup',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'melt',\n",
       " 'memory_usage',\n",
       " 'merge',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'pivot',\n",
       " 'pivot_table',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'query',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'rdiv',\n",
       " 'reindex',\n",
       " 'reindex_axis',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'select',\n",
       " 'select_dtypes',\n",
       " 'sem',\n",
       " 'sentiment',\n",
       " 'set_axis',\n",
       " 'set_index',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'slice_shift',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'style',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tag',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dense',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_feather',\n",
       " 'to_gbq',\n",
       " 'to_hdf',\n",
       " 'to_html',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_msgpack',\n",
       " 'to_panel',\n",
       " 'to_parquet',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_records',\n",
       " 'to_sparse',\n",
       " 'to_sql',\n",
       " 'to_stata',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tshift',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'values',\n",
       " 'var',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdb;pdb.set_trace()\n",
    "dir(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the NaN \n",
    "full_data[\"Subject\"] = full_data[\"Subject\"].fillna('')\n",
    "full_data[\"Description\"] = full_data[\"Description\"].fillna('')\n",
    "full_data[\"Sentiment\"] = full_data[\"sentiment\"].fillna(0)\n",
    "#full_data[\"tag\"] = full_data[\"tag\"].fillna('')\n",
    "# full_data[\"Developer Reply Date and Time\"] = full_data[\"Developer Reply Date and Time\"].fillna('')\n",
    "# full_data[\"Developer Reply Millis Since Epoch\"] = full_data[\"Developer Reply Millis Since Epoch\"].fillna('')\n",
    "# full_data[\"Developer Reply Text\"] = full_data[\"Developer Reply Text\"].fillna('')\n",
    "\n",
    "# slicing the required columns only\n",
    "full_data = full_data[[\"Subject\", \"Description\", \"sentiment\", \"tag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean mail data\n",
    "#mail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.loc[full_data['sentiment'] == 0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product!</td>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[\" Please start for men's asap\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brilliant concept :)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Subject                                        Description  \\\n",
       "0  Great product!  This is a brilliant concept. Please start for ...   \n",
       "1                                               Brilliant concept :)   \n",
       "2                                                                      \n",
       "3                            Brilliant stuff!! Waiting to see more..   \n",
       "4                                                                      \n",
       "\n",
       "   sentiment                               tag  \n",
       "0          2  [\" Please start for men's asap\"]  \n",
       "1          2          ['Brilliant concept :)']  \n",
       "2         -1                                []  \n",
       "3          1                              ['']  \n",
       "4         -1                                []  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full_data.columns = [\"Description\",\"Subject\"]\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mail_data = mail_data[[\"subject\", \"description\"]]\n",
    "# mail_data.columns = [\"Subject\", \"Description\"]\n",
    "# mail_data.head()\n",
    "\n",
    "# when mail data get cleaned properly then use the below\n",
    "#full_data = full_data.append(mail_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td></td>\n",
       "      <td>Wow! Never knew that it might be this smooth t...</td>\n",
       "      <td>2</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td></td>\n",
       "      <td>A huge thank you for making it so easy and fun...</td>\n",
       "      <td>2</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td></td>\n",
       "      <td>It is very good app</td>\n",
       "      <td>2</td>\n",
       "      <td>['It is very good app']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td></td>\n",
       "      <td>It is so higher to lease cloths! Facilitates m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[' So higher and high-quality']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td></td>\n",
       "      <td>It's miles higher manner to lease cloths. I am...</td>\n",
       "      <td>2</td>\n",
       "      <td>[\"It's miles higher manner to lease cloths\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject                                        Description  sentiment  \\\n",
       "995          Wow! Never knew that it might be this smooth t...          2   \n",
       "996          A huge thank you for making it so easy and fun...          2   \n",
       "997                                        It is very good app          2   \n",
       "998          It is so higher to lease cloths! Facilitates m...          1   \n",
       "999          It's miles higher manner to lease cloths. I am...          2   \n",
       "\n",
       "                                              tag  \n",
       "995                                          ['']  \n",
       "996                                          ['']  \n",
       "997                       ['It is very good app']  \n",
       "998               [' So higher and high-quality']  \n",
       "999  [\"It's miles higher manner to lease cloths\"]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product!</td>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[\" Please start for men's asap\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brilliant concept :)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>This idea will be considered as one of the top...</td>\n",
       "      <td>2</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brilliant ..this is what we needed!!</td>\n",
       "      <td>No spending on buying apparels for gf... Why t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[' Why to buy if u can rent it']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Subject  \\\n",
       "0                        Great product!   \n",
       "1                                         \n",
       "3                                         \n",
       "5                             Excellent   \n",
       "7  Brilliant ..this is what we needed!!   \n",
       "\n",
       "                                         Description  sentiment  \\\n",
       "0  This is a brilliant concept. Please start for ...          2   \n",
       "1                               Brilliant concept :)          2   \n",
       "3            Brilliant stuff!! Waiting to see more..          1   \n",
       "5  This idea will be considered as one of the top...          2   \n",
       "7  No spending on buying apparels for gf... Why t...          1   \n",
       "\n",
       "                                tag  \n",
       "0  [\" Please start for men's asap\"]  \n",
       "1          ['Brilliant concept :)']  \n",
       "3                              ['']  \n",
       "5                              ['']  \n",
       "7  [' Why to buy if u can rent it']  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = full_data.loc[full_data['Description'] != '']\n",
    "\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8669"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d05ff8683c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentiment_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mfull_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4852\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4853\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4854\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4855\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4856\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4948\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4949\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4950\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4951\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4952\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-d05ff8683c8f>\u001b[0m in \u001b[0;36mget_sentiment_score\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Detects the sentiment of the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmin_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/google/cloud/language_v1/gapic/language_service_client.py\u001b[0m in \u001b[0;36manalyze_sentiment\u001b[0;34m(self, document, encoding_type, retry, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         request = language_service_pb2.AnalyzeSentimentRequest(\n\u001b[1;32m    179\u001b[0m             document=document, encoding_type=encoding_type)\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     def analyze_entities(self,\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;34m\"\"\"Wrapped function that adds timeout.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timeout'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         state, call, deadline = self._blocking(request, timeout, metadata,\n\u001b[0;32m--> 491\u001b[0;31m                                                credentials)\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_blocking\u001b[0;34m(self, request, timeout, metadata, credentials)\u001b[0m\n\u001b[1;32m    483\u001b[0m                 cygrpc.Operations(operations), None)\n\u001b[1;32m    484\u001b[0m             \u001b[0m_check_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             _handle_event(completion_queue.poll(), state,\n\u001b[0m\u001b[1;32m    486\u001b[0m                           self._response_deserializer)\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.CompletionQueue.poll\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# add a named entities to look for\n",
    "#import pdb;pdb.set_trace()\n",
    "negation_list = [\"No\", \"Not\"]\n",
    "\n",
    "tech_list = [\"order\", \"placed\", \"tech\", \"issue\", \"coupon\", \"item\", \"order placed\", \"coupon issue\", \"tech issue\", \"App crash\", \"web crash\", \"ios\", \n",
    "            \"android\", \"signup issue\", \"signin issue\", \"order placed\", \"order confirmation\", \"online payment\", \"hang\", \"too slow\"\n",
    "            , \"cart\", \"image not matching\"]\n",
    "ops_list = [\"delivery\", \"measurement\", \"pickup\", \"refund\",  \"fitting\", \"trial\", \"pickup\", \"refund\", \"deposit\", \"on-time\", \"delay\", \"address updation\", \"return\", \"cancellation\", \"exchange\",\n",
    "\"payment issues\"]\n",
    "product_list = [\"content\", \"available\", \"error\", \"loading\", \"website not loading\", \"site not loading\", \"content not available\", \"cloth tear\", \"defective product\", \"discount codes\"]\n",
    "\n",
    "def get_sentiment_score(item):\n",
    "    sentences = item['Description'].split('.')#.encode('utf-16')\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #a = b\n",
    "    sentiment = None\n",
    "    sentence_dict = {}\n",
    "    min_score = 100\n",
    "    min_text_tag_list = []\n",
    "    for text in sentences:\n",
    "        document = types.Document(\n",
    "            content=text,\n",
    "            type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "        # Detects the sentiment of the text\n",
    "        \n",
    "        sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "        if sentiment.score < min_score:\n",
    "            min_score = sentiment.score\n",
    "            min_text_tag_list.append(text) \n",
    "        sentence_dict.update({text: sentiment.score})\n",
    "        \n",
    "    sentiment_score= sum(sentence_dict.values()) / len(sentence_dict.values())\n",
    "    if sentiment_score <= -0.25 and sentiment_score > -1.0:\n",
    "        sentiment = 0\n",
    "    elif sentiment_score <= 0.25 and sentiment_score > -0.25:\n",
    "        sentiment = 1\n",
    "    elif sentiment_score <= 1.00 and sentiment_score > 0.25:\n",
    "        sentiment = 2\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     negate = None\n",
    "#     for element in negation_list:\n",
    "#         m = re.findall('^'+element+' \\w+', sentences)\n",
    "#         if m:\n",
    "#             break\n",
    "#     if m:\n",
    "#         negate, sentences = sentences[:2],sentences[2:]\n",
    "#     ss = sid.polarity_scores(sentences)\n",
    "#     if negate:\n",
    "#         if ss['neg'] > max(ss['pos'], ss['neu']): \n",
    "#             ss['pos'] = 1.0\n",
    "#             ss['neu'] = 0.0\n",
    "#             ss['neg'] = 0.0\n",
    "#         elif ss['pos'] > max(ss['neg'], ss['neu']): \n",
    "#             ss['pos'] = 0.0\n",
    "#             ss['neu'] = 0.0\n",
    "#             ss['neg'] = 1.0\n",
    "# #         elif ss['neu'] > max(ss['pos'], ss['neg']):\n",
    "# #             ss['pos'] = 1.0\n",
    "# #             ss['neu'] = 0.0\n",
    "# #             ss['neg'] = 0.0\n",
    "    \n",
    "#     negative = ss[\"neg\"]\n",
    "#     positive = ss[\"pos\"]\n",
    "#     neutral = ss[\"neu\"]\n",
    "    #compound = ss[\"compound\"]\n",
    "    return pd.Series({'sentiment': sentiment, \"tag\": min_text_tag_list})\n",
    "\n",
    "\n",
    "scores = full_data.apply(get_sentiment_score, axis=1)\n",
    "\n",
    "final_data = pd.concat([ full_data , scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sentiment_class(item):\n",
    "#     sentiment_class = 0\n",
    "#     score_list = [item['neutral'], item['negative'], item['positive']]\n",
    "#     max_val = max(score_list)\n",
    "    \n",
    "#     sentiment_class = score_list.index(max_val)\n",
    "    \n",
    "#     return pd.Series({'sentiment_class': sentiment_class})\n",
    "\n",
    "\n",
    "# score_class = final_data.apply(get_sentiment_class, axis=1)\n",
    "\n",
    "# final_sliced_data = pd.concat([ final_data , score_class], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the data with probability tech + , tech - , ops + , ops - , product + , product -\n",
    "import re\n",
    "tech_list = [\"order\", \"placed\", \"tech\", \"issue\", \"coupon\", \"item\", \"order placed\", \"coupon issue\", \"tech issue\", \"App crash\", \"web crash\", \"ios\", \n",
    "            \"android\", \"signup issue\", \"signin issue\", \"order placed\", \"order confirmation\", \"online payment\", \"hang\", \"too slow\"\n",
    "            , \"cart\", \"image not matching\"]\n",
    "ops_list = [\"delivery\", \"measurement\", \"pickup\", \"refund\",  \"fitting\", \"trial\", \"pickup\", \"refund\", \"deposit\", \"on-time\", \"delay\", \"address updation\", \"return\", \"cancellation\", \"exchange\",\n",
    "\"payment issues\"]\n",
    "product_list = [\"content\", \"available\", \"error\", \"loading\", \"website not loading\", \"site not loading\", \"content not available\", \"cloth tear\", \"defective product\", \"discount codes\"]\n",
    "\n",
    "def calculate_token_probability(item):\n",
    "    tech = 0; ops = 0; product = 0\n",
    "    tokens = item['Description'].split(' ')\n",
    "#     for element in tokens:\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         try:\n",
    "#             tech_match = re.findall(element.lower(), tech_str)\n",
    "#         except Exception:\n",
    "#             tech_match = None\n",
    "            \n",
    "#         try:\n",
    "#             ops_match = re.findall(element.lower(), ops_str)\n",
    "#         except Exception:\n",
    "#             ops_match = None\n",
    "            \n",
    "#         try:\n",
    "#             product_match = re.findall(element.lower(), product_str)\n",
    "#         except Exception:\n",
    "#             product_match = None\n",
    "#         if tech_match:\n",
    "#             tech += 1\n",
    "#         elif ops_match:\n",
    "#             ops += 1\n",
    "#         elif product_match:\n",
    "#             product += 1\n",
    "    tech = [1 for tech_i in tech_list if tech_i.lower() in item['Description']]\n",
    "    ops = [1 for ops_i in ops_list if ops_i.lower() in item['Description']]\n",
    "    product = [1 for product_i in product_list if product_i.lower() in item['Description']]\n",
    "    \n",
    "\n",
    "#     for tech_i in tech_list:\n",
    "#         try:\n",
    "#             tech_match = re.findall(tech_i.lower(), item['Review Text'])\n",
    "#         except Exception:\n",
    "#             tech_match = None\n",
    "            \n",
    "#         if tech_match:\n",
    "#              tech += 1\n",
    "    \n",
    "#     for ops_i in ops_list:\n",
    "#         try:\n",
    "#             ops_match = re.findall(ops_i.lower(), item['Review Text'])\n",
    "#         except Exception:\n",
    "#             ops_match = None\n",
    "            \n",
    "#         if ops_match:\n",
    "#              ops += 1\n",
    "    \n",
    "#     for product_i in product_list:\n",
    "#         try:\n",
    "#             product_match = re.findall(product_i.lower(), item['Review Text'])\n",
    "#         except Exception:\n",
    "#             product_match = None\n",
    "            \n",
    "#         if product_match:\n",
    "#              product += 1\n",
    "            \n",
    "    tech_sum = sum(tech)\n",
    "    ops_sum = sum(ops)\n",
    "    product_sum = sum(product)\n",
    "    \n",
    "    sum_all = tech_sum + ops_sum + product_sum\n",
    "    try:\n",
    "        return pd.Series({'tech_prob': tech_sum/sum_all, 'ops_prob': ops_sum/sum_all, 'product_prob': product_sum/sum_all})\n",
    "    except Exception: \n",
    "        return pd.Series({'tech_prob': 0, 'ops_prob': 0, 'product_prob': 0})\n",
    "    \n",
    "    \n",
    "# def cal(item):\n",
    "#     print(item['Review Text'])\n",
    "#     return pd.Series({'hello': item}, )\n",
    "    \n",
    "# probability = sliced_data.apply(calculate_token_probability, axis=1)\n",
    "\n",
    "# final_sliced_data = pd.concat([ sliced_data , probability], axis=1)\n",
    "\n",
    "\n",
    "# final_sliced_data['ops_positive'] = (( final_sliced_data['ops_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Positive\"] == True).astype(int).values\n",
    "# final_sliced_data['product_positive'] = (( final_sliced_data['product_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Positive\"] == True ).astype(int).values\n",
    "# final_sliced_data['tech_positive'] = (( final_sliced_data['tech_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1)  ) & final_sliced_data[\"Positive\"] == True).astype(int).values\n",
    "# final_sliced_data['ops_negative'] = (( final_sliced_data['ops_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Negative\"] == True).astype(int).values\n",
    "# final_sliced_data['product_negative'] = (( final_sliced_data['product_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Negative\"] == True).astype(int).values\n",
    "# final_sliced_data['tech_negative'] = (( final_sliced_data['tech_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Negative\"] == True).astype(int).values\n",
    "# final_sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is a brilliant concept. Please start for ...\n",
       "1                                 Brilliant concept :)\n",
       "3              Brilliant stuff!! Waiting to see more..\n",
       "5    This idea will be considered as one of the top...\n",
       "7    No spending on buying apparels for gf... Why t...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sliced_data['Description'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Subject</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.872</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This idea will be considered as one of the top...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No spending on buying apparels for gf... Why t...</td>\n",
       "      <td>Brilliant ..this is what we needed!!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  This is a brilliant concept. Please start for ...   \n",
       "1                               Brilliant concept :)   \n",
       "3            Brilliant stuff!! Waiting to see more..   \n",
       "5  This idea will be considered as one of the top...   \n",
       "7  No spending on buying apparels for gf... Why t...   \n",
       "\n",
       "                                Subject  negative  neutral  positive  \\\n",
       "0                        Great product!       0.0    0.534     0.466   \n",
       "1                                             0.0    0.128     0.872   \n",
       "3                                             0.0    0.533     0.467   \n",
       "5                             Excellent       0.0    0.715     0.285   \n",
       "7  Brilliant ..this is what we needed!!       0.0    1.000     0.000   \n",
       "\n",
       "   sentiment_class  \n",
       "0                0  \n",
       "1                2  \n",
       "3                0  \n",
       "5                0  \n",
       "7                0  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sliced_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate csv file\n",
    "final_sliced_data.to_csv(\"text_data.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sliced_data.to_csv('pos_neg_score.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999\n",
      "30\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "print(final_sliced_data.loc[final_sliced_data['sentiment_class'] == 0].__len__())\n",
    "print(final_sliced_data.loc[final_sliced_data['sentiment_class'] == 1].__len__())\n",
    "print(final_sliced_data.loc[final_sliced_data['sentiment_class'] == 2].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in word_tokenize(sent)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-b51f8469f8f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, GaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mnbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "labels = final_sliced_data[\"sentiment_class\"].values;\n",
    "\n",
    "#import pdb;pdb.set_trace()  \n",
    "kf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "\n",
    "totalNBV = 0\n",
    "\n",
    "totalMatNBV = np.zeros((3,3));\n",
    "\n",
    "# tr, te = kf.split(sliced_data['Review Text'], labels)\n",
    "# td = []\n",
    "# for i in train_index:\n",
    "#     td.append(sliced_data['Review Text'][i])\n",
    "#import pdb;pdb.set_trace()\n",
    "review_text = final_sliced_data['Description'].tolist()\n",
    "\n",
    "# break words\n",
    "review_words = []\n",
    "\n",
    "# for item in review_text:\n",
    "#     review_words += item.split(\" \")\n",
    "counter = 0\n",
    "for train_index, test_index in kf.split(review_text, labels):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(train_index.__len__(), test_index.__len__())\n",
    "    # X_train = [review_text[i] for i in train_index]\n",
    "    # X_test = [review_text[i] for i in test_index]\n",
    "    #import pdb;pdb.set_trace()\n",
    "    X_train = [[format_sentence(review_text[i]), int(final_sliced_data.iloc[i]['sentiment_class'])] for i in train_index]\n",
    "    X_test = [[format_sentence(review_text[i]), int(final_sliced_data.iloc[i]['sentiment_class'])] for i in test_index]\n",
    "    \n",
    "    X_train_nb = [[list(map(int, format_sentence(review_text[i]).values())), int(final_sliced_data.iloc[i]['sentiment_class'])] for i in train_index]\n",
    "    X_test_nb = [[list(map(int, format_sentence(review_text[i]).values())), int(final_sliced_data.iloc[i]['sentiment_class'])] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    #vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=False, stop_words='english')\n",
    "    #train_corpus_tf_idf = vectorizer.fit_transform(X_train) \n",
    "    #test_corpus_tf_idf = vectorizer.transform(X_test)\n",
    "    nb = MultinomialNB()#, GaussianNB\n",
    "    #import pdb;pdb.set_trace()\n",
    "    nb.fit(X_train_nb, y_train)\n",
    "    nb.predict(X_test_nb)\n",
    "    nbc = NaiveBayesClassifier.train(X_train)\n",
    "    print(accuracy(nbc, X_test))\n",
    "    print(accuracy(nb, X_test_nb))\n",
    "    \n",
    "    example1 = \"Cats are awesome!\"\n",
    " \n",
    "    print(nbc.classify(format_sentence(example1)))\n",
    "    \n",
    "    example2 = \"I dont like cats.\"\n",
    " \n",
    "    print(nbc.classify(format_sentence(example2)))\n",
    "    result1 = []\n",
    "    for item in X_test:\n",
    "        result1.append(nbc.classify(item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sid.polarity_scores(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
