{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/chitrankdixit/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import csv\n",
    "import sys,os\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "\n",
    "# for named entity\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, tree, download\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.porter import *\n",
    "# for preparing data\n",
    "download('vader_lexicon')\n",
    "%matplotlib inline\n",
    "\n",
    "#!export GOOGLE_APPLICATION_CREDENTIALS='sentimental.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='sentimental.json'\n",
    "\n",
    "# nltk for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'./playstore_corpus_tsv/' # use your path\n",
    "allFiles = glob.glob(path + \"/*.tsv\")\n",
    "\n",
    "# for out_count, file in enumerate(allFiles):\n",
    "#     with open(file, newline='') as csvfile:\n",
    "#         spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         for counter, row in enumerate(spamreader):\n",
    "#             import pdb;pdb.set_trace()\n",
    "#             print(row)\n",
    "            \n",
    "#     with open(file+out_count, 'wt') as csvfile:\n",
    "#             writer = csv.writer(csvfile, delimiter='', quotechar='|')\n",
    "#             #writer.writerow([\"#\"] + anarkali_characteristics)\n",
    "#             #import pdb;pdb.set_trace()\n",
    "#             for element in lehenga_characteristics:\n",
    "#                 #import pdb;pdb.set_trace()\n",
    "#                 writer.writerow([element[\"name\"], element[\"count\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full data\n",
    "\n",
    "\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "\n",
    "# datacolumns = [\n",
    "#     \"Package Name\",\n",
    "#     \"App Version Code\",\n",
    "#     \"Reviewer Language\",\n",
    "#     \"Device\",\n",
    "#     \"Review Submit Date and Time\",\n",
    "#     \"Review Submit Millis Since Epoch\",\n",
    "#     \"Review Last Update Date and Time\",\n",
    "#     \"Review Last Update Millis Since Epoch\",\n",
    "#     \"Star Rating\",\n",
    "#     \"Review Title\",\n",
    "#     \"Review Text\",\n",
    "#     \"Developer Reply Date and Time\",\n",
    "#     \"Developer Reply Millis Since Epoch\",\n",
    "#     \"Developer Reply Text,Review Link\"\n",
    "# ]\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, encoding = \"ISO-8859-1\", error_bad_lines=False, sep='\\t')\n",
    "    list_.append(df)\n",
    "    \n",
    "    \n",
    "full_data = pd.concat(list_)\n",
    "full_data.head(5)\n",
    "\n",
    "cleaned_data = pd.read_csv(\"pos_neg_score2_class_mix.tsv\",index_col=None, header=0, encoding = \"ISO-8859-1\", error_bad_lines=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the NaN \n",
    "full_data[\"Subject\"] = full_data[\"Subject\"].fillna('')\n",
    "full_data[\"Description\"] = full_data[\"Description\"].fillna('')\n",
    "full_data[\"Sentiment\"] = full_data[\"sentiment\"].fillna(0)\n",
    "#full_data[\"tag\"] = full_data[\"tag\"].fillna('')\n",
    "# full_data[\"Developer Reply Date and Time\"] = full_data[\"Developer Reply Date and Time\"].fillna('')\n",
    "# full_data[\"Developer Reply Millis Since Epoch\"] = full_data[\"Developer Reply Millis Since Epoch\"].fillna('')\n",
    "# full_data[\"Developer Reply Text\"] = full_data[\"Developer Reply Text\"].fillna('')\n",
    "\n",
    "# slicing the required columns only\n",
    "full_data = full_data[[\"Subject\", \"Description\", \"sentiment\", \"tag\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean mail data\n",
    "#mail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.loc[full_data['sentiment'] == 0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product!</td>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[\" Please start for men's asap\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brilliant concept :)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Subject                                        Description  \\\n",
       "0  Great product!  This is a brilliant concept. Please start for ...   \n",
       "1                                               Brilliant concept :)   \n",
       "2                                                                      \n",
       "3                            Brilliant stuff!! Waiting to see more..   \n",
       "4                                                                      \n",
       "\n",
       "   sentiment                               tag  \n",
       "0          2  [\" Please start for men's asap\"]  \n",
       "1          2          ['Brilliant concept :)']  \n",
       "2         -1                                []  \n",
       "3          1                              ['']  \n",
       "4         -1                                []  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full_data.columns = [\"Description\",\"Subject\"]\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mail_data = mail_data[[\"subject\", \"description\"]]\n",
    "# mail_data.columns = [\"Subject\", \"Description\"]\n",
    "# mail_data.head()\n",
    "\n",
    "# when mail data get cleaned properly then use the below\n",
    "#full_data = full_data.append(mail_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td></td>\n",
       "      <td>Wow! Never knew that it might be this smooth t...</td>\n",
       "      <td>2</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td></td>\n",
       "      <td>A huge thank you for making it so easy and fun...</td>\n",
       "      <td>2</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td></td>\n",
       "      <td>It is very good app</td>\n",
       "      <td>2</td>\n",
       "      <td>['It is very good app']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td></td>\n",
       "      <td>It is so higher to lease cloths! Facilitates m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[' So higher and high-quality']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td></td>\n",
       "      <td>It's miles higher manner to lease cloths. I am...</td>\n",
       "      <td>2</td>\n",
       "      <td>[\"It's miles higher manner to lease cloths\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject                                        Description  sentiment  \\\n",
       "995          Wow! Never knew that it might be this smooth t...          2   \n",
       "996          A huge thank you for making it so easy and fun...          2   \n",
       "997                                        It is very good app          2   \n",
       "998          It is so higher to lease cloths! Facilitates m...          1   \n",
       "999          It's miles higher manner to lease cloths. I am...          2   \n",
       "\n",
       "                                              tag  \n",
       "995                                          ['']  \n",
       "996                                          ['']  \n",
       "997                       ['It is very good app']  \n",
       "998               [' So higher and high-quality']  \n",
       "999  [\"It's miles higher manner to lease cloths\"]  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product!</td>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[\" Please start for men's asap\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brilliant concept :)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>1</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>This idea will be considered as one of the top...</td>\n",
       "      <td>2</td>\n",
       "      <td>['']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brilliant ..this is what we needed!!</td>\n",
       "      <td>No spending on buying apparels for gf... Why t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[' Why to buy if u can rent it']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Subject  \\\n",
       "0                        Great product!   \n",
       "1                                         \n",
       "3                                         \n",
       "5                             Excellent   \n",
       "7  Brilliant ..this is what we needed!!   \n",
       "\n",
       "                                         Description  sentiment  \\\n",
       "0  This is a brilliant concept. Please start for ...          2   \n",
       "1                               Brilliant concept :)          2   \n",
       "3            Brilliant stuff!! Waiting to see more..          1   \n",
       "5  This idea will be considered as one of the top...          2   \n",
       "7  No spending on buying apparels for gf... Why t...          1   \n",
       "\n",
       "                                tag  \n",
       "0  [\" Please start for men's asap\"]  \n",
       "1          ['Brilliant concept :)']  \n",
       "3                              ['']  \n",
       "5                              ['']  \n",
       "7  [' Why to buy if u can rent it']  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = full_data.loc[full_data['Description'] != '']\n",
    "\n",
    "tags = full_data[[\"tag\"]]\n",
    "full_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8669, 8669, 8669)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data), len(tags), len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Subject</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This idea will be considered as one of the top...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No spending on buying apparels for gf... Why t...</td>\n",
       "      <td>Brilliant ..this is what we needed!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  This is a brilliant concept. Please start for ...   \n",
       "1                               Brilliant concept :)   \n",
       "2            Brilliant stuff!! Waiting to see more..   \n",
       "3  This idea will be considered as one of the top...   \n",
       "4  No spending on buying apparels for gf... Why t...   \n",
       "\n",
       "                                Subject  sentiment  \n",
       "0                        Great product!          1  \n",
       "1                                   NaN          1  \n",
       "2                                   NaN          1  \n",
       "3                             Excellent          1  \n",
       "4  Brilliant ..this is what we needed!!          0  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8669"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# add a named entities to look for\n",
    "#import pdb;pdb.set_trace()\n",
    "negation_list = [\"No\", \"Not\"]\n",
    "\n",
    "tech_list = [\"order\", \"placed\", \"tech\", \"issue\", \"coupon\", \"item\", \"order placed\", \"coupon issue\", \"tech issue\", \"App crash\", \"web crash\", \"ios\", \n",
    "            \"android\", \"signup issue\", \"signin issue\", \"order placed\", \"order confirmation\", \"online payment\", \"hang\", \"too slow\"\n",
    "            , \"cart\", \"image not matching\"]\n",
    "ops_list = [\"delivery\", \"measurement\", \"pickup\", \"refund\",  \"fitting\", \"trial\", \"pickup\", \"refund\", \"deposit\", \"on-time\", \"delay\", \"address updation\", \"return\", \"cancellation\", \"exchange\",\n",
    "\"payment issues\"]\n",
    "product_list = [\"content\", \"available\", \"error\", \"loading\", \"website not loading\", \"site not loading\", \"content not available\", \"cloth tear\", \"defective product\", \"discount codes\"]\n",
    "\n",
    "def get_sentiment_score(item):\n",
    "    negate = None\n",
    "    for sentence in item['Description'].split('.'):\n",
    "        for element in negation_list:\n",
    "            m = re.findall('^'+element+' \\w+', sentence)\n",
    "            if m:\n",
    "                break\n",
    "    if m:\n",
    "        negate, sentence = sentence[:2],sentence[2:]\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    if negate:\n",
    "        vs['pos'] = 1.0\n",
    "        vs['neu'] = 0.0\n",
    "        vs['neg'] = 0.0\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "#     sentences = item['Description'].split('.')#.encode('utf-16')\n",
    "#     #import pdb;pdb.set_trace()\n",
    "#     #a = b\n",
    "#     sentiment = None\n",
    "#     sentence_dict = {}\n",
    "#     min_score = 100\n",
    "#     min_text_tag_list = []\n",
    "#     for text in sentences:\n",
    "#         document = types.Document(\n",
    "#             content=text,\n",
    "#             type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "#         # Detects the sentiment of the text\n",
    "        \n",
    "#         sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "#         if sentiment.score < min_score:\n",
    "#             min_score = sentiment.score\n",
    "#             min_text_tag_list.append(text) \n",
    "#         sentence_dict.update({text: sentiment.score})\n",
    "        \n",
    "#     sentiment_score= sum(sentence_dict.values()) / len(sentence_dict.values())\n",
    "#     if sentiment_score <= -0.25 and sentiment_score > -1.0:\n",
    "#         sentiment = 0\n",
    "#     elif sentiment_score <= 0.25 and sentiment_score > -0.25:\n",
    "#         sentiment = 1\n",
    "#     elif sentiment_score <= 1.00 and sentiment_score > 0.25:\n",
    "#         sentiment = 2\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     negate = None\n",
    "#     for element in negation_list:\n",
    "#         m = re.findall('^'+element+' \\w+', sentences)\n",
    "#         if m:\n",
    "#             break\n",
    "#     if m:\n",
    "#         negate, sentences = sentences[:2],sentences[2:]\n",
    "#     ss = sid.polarity_scores(sentences)\n",
    "#     if negate:\n",
    "#         if ss['neg'] > max(ss['pos'], ss['neu']): \n",
    "#             ss['pos'] = 1.0\n",
    "#             ss['neu'] = 0.0\n",
    "#             ss['neg'] = 0.0\n",
    "#         elif ss['pos'] > max(ss['neg'], ss['neu']): \n",
    "#             ss['pos'] = 0.0\n",
    "#             ss['neu'] = 0.0\n",
    "#             ss['neg'] = 1.0\n",
    "# #         elif ss['neu'] > max(ss['pos'], ss['neg']):\n",
    "# #             ss['pos'] = 1.0\n",
    "# #             ss['neu'] = 0.0\n",
    "# #             ss['neg'] = 0.0\n",
    "    \n",
    "#     negative = ss[\"neg\"]\n",
    "#     positive = ss[\"pos\"]\n",
    "#     neutral = ss[\"neu\"]\n",
    "    #compound = ss[\"compound\"]\n",
    "    return pd.Series({'sentiment': sentiment, \"tag\": min_text_tag_list})\n",
    "\n",
    "\n",
    "# scores = full_data.apply(get_sentiment_score, axis=1)\n",
    "\n",
    "# final_data = pd.concat([ full_data , scores], axis=1)\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (4, 16338), indices imply (4, 8669)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-989dc985a199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#cleaned_data['tags'] = tags['tag']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcleaned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mcleaned_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                        copy=copy)\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5200\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   3026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3028\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3030\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3239\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3241\u001b[0m             raise AssertionError('Number of manager items must equal union of '\n",
      "\u001b[0;32m~/Documents/work/sentimental_analysis/sentimental/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4601\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4602\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4603\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (4, 16338), indices imply (4, 8669)"
     ]
    }
   ],
   "source": [
    "# def get_sentiment_class(item):\n",
    "#     sentiment_class = 0\n",
    "#     score_list = [item['neutral'], item['negative'], item['positive']]\n",
    "#     max_val = max(score_list)\n",
    "    \n",
    "#     sentiment_class = score_list.index(max_val)\n",
    "    \n",
    "#     return pd.Series({'sentiment_class': sentiment_class})\n",
    "\n",
    "\n",
    "# score_class = final_data.apply(get_sentiment_class, axis=1)\n",
    "\n",
    "# final_sliced_data = pd.concat([ final_data , score_class], axis=1)\n",
    "len(cleaned_data), len(tags)\n",
    "#cleaned_data['tags'] = tags['tag']\n",
    "\n",
    "cleaned_data = pd.concat([ cleaned_data , tags], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the data with probability tech + , tech - , ops + , ops - , product + , product -\n",
    "import re\n",
    "tech_list = [\"order\", \"placed\", \"tech\", \"issue\", \"coupon\", \"item\", \"order placed\", \"coupon issue\", \"tech issue\", \"App crash\", \"web crash\", \"ios\", \n",
    "            \"android\", \"signup issue\", \"signin issue\", \"order placed\", \"order confirmation\", \"online payment\", \"hang\", \"too slow\"\n",
    "            , \"cart\", \"image not matching\"]\n",
    "ops_list = [\"delivery\", \"measurement\", \"pickup\", \"refund\",  \"fitting\", \"trial\", \"pickup\", \"refund\", \"deposit\", \"on-time\", \"delay\", \"address updation\", \"return\", \"cancellation\", \"exchange\",\n",
    "\"payment issues\"]\n",
    "product_list = [\"content\", \"available\", \"error\", \"loading\", \"website not loading\", \"site not loading\", \"content not available\", \"cloth tear\", \"defective product\", \"discount codes\"]\n",
    "\n",
    "def calculate_token_probability(item):\n",
    "    tech = 0; ops = 0; product = 0\n",
    "    tokens = item['Description'].split(' ')\n",
    "#     for element in tokens:\n",
    "#         #import pdb;pdb.set_trace()\n",
    "#         try:\n",
    "#             tech_match = re.findall(element.lower(), tech_str)\n",
    "#         except Exception:\n",
    "#             tech_match = None\n",
    "            \n",
    "#         try:\n",
    "#             ops_match = re.findall(element.lower(), ops_str)\n",
    "#         except Exception:\n",
    "#             ops_match = None\n",
    "            \n",
    "#         try:\n",
    "#             product_match = re.findall(element.lower(), product_str)\n",
    "#         except Exception:\n",
    "#             product_match = None\n",
    "#         if tech_match:\n",
    "#             tech += 1\n",
    "#         elif ops_match:\n",
    "#             ops += 1\n",
    "#         elif product_match:\n",
    "#             product += 1\n",
    "    tech = [1 for tech_i in tech_list if tech_i.lower() in item['Description']]\n",
    "    ops = [1 for ops_i in ops_list if ops_i.lower() in item['Description']]\n",
    "    product = [1 for product_i in product_list if product_i.lower() in item['Description']]\n",
    "    \n",
    "\n",
    "#     for tech_i in tech_list:\n",
    "#         try:\n",
    "#             tech_match = re.findall(tech_i.lower(), item['Review Text'])\n",
    "#         except Exception:\n",
    "#             tech_match = None\n",
    "            \n",
    "#         if tech_match:\n",
    "#              tech += 1\n",
    "    \n",
    "#     for ops_i in ops_list:\n",
    "#         try:\n",
    "#             ops_match = re.findall(ops_i.lower(), item['Review Text'])\n",
    "#         except Exception:\n",
    "#             ops_match = None\n",
    "            \n",
    "#         if ops_match:\n",
    "#              ops += 1\n",
    "    \n",
    "#     for product_i in product_list:\n",
    "#         try:\n",
    "#             product_match = re.findall(product_i.lower(), item['Review Text'])\n",
    "#         except Exception:\n",
    "#             product_match = None\n",
    "            \n",
    "#         if product_match:\n",
    "#              product += 1\n",
    "            \n",
    "    tech_sum = sum(tech)\n",
    "    ops_sum = sum(ops)\n",
    "    product_sum = sum(product)\n",
    "    \n",
    "    sum_all = tech_sum + ops_sum + product_sum\n",
    "    try:\n",
    "        return pd.Series({'tech_prob': tech_sum/sum_all, 'ops_prob': ops_sum/sum_all, 'product_prob': product_sum/sum_all})\n",
    "    except Exception: \n",
    "        return pd.Series({'tech_prob': 0, 'ops_prob': 0, 'product_prob': 0})\n",
    "    \n",
    "    \n",
    "# def cal(item):\n",
    "#     print(item['Review Text'])\n",
    "#     return pd.Series({'hello': item}, )\n",
    "    \n",
    "# probability = sliced_data.apply(calculate_token_probability, axis=1)\n",
    "\n",
    "# final_sliced_data = pd.concat([ sliced_data , probability], axis=1)\n",
    "\n",
    "\n",
    "# final_sliced_data['ops_positive'] = (( final_sliced_data['ops_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Positive\"] == True).astype(int).values\n",
    "# final_sliced_data['product_positive'] = (( final_sliced_data['product_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Positive\"] == True ).astype(int).values\n",
    "# final_sliced_data['tech_positive'] = (( final_sliced_data['tech_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1)  ) & final_sliced_data[\"Positive\"] == True).astype(int).values\n",
    "# final_sliced_data['ops_negative'] = (( final_sliced_data['ops_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Negative\"] == True).astype(int).values\n",
    "# final_sliced_data['product_negative'] = (( final_sliced_data['product_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Negative\"] == True).astype(int).values\n",
    "# final_sliced_data['tech_negative'] = (( final_sliced_data['tech_prob'] == final_sliced_data[['ops_prob', 'tech_prob', 'product_prob']].max(axis=1) ) & final_sliced_data[\"Negative\"] == True).astype(int).values\n",
    "# final_sliced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = cleaned_data.loc[(cleaned_data['sentiment'] == 0)][:999]\n",
    "positive_data = cleaned_data.loc[(cleaned_data['sentiment'] == 1)][:999]\n",
    "\n",
    "cleaned_data = positive_data.append(negative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "5       1\n",
       "6       1\n",
       "7       1\n",
       "10      1\n",
       "12      1\n",
       "13      1\n",
       "14      1\n",
       "16      1\n",
       "17      1\n",
       "18      1\n",
       "19      1\n",
       "21      1\n",
       "23      1\n",
       "25      1\n",
       "26      1\n",
       "28      1\n",
       "29      1\n",
       "30      1\n",
       "32      1\n",
       "33      1\n",
       "34      1\n",
       "35      1\n",
       "36      1\n",
       "37      1\n",
       "38      1\n",
       "39      1\n",
       "       ..\n",
       "8609    0\n",
       "8611    0\n",
       "8612    0\n",
       "8613    0\n",
       "8615    0\n",
       "8617    0\n",
       "8620    0\n",
       "8621    0\n",
       "8622    0\n",
       "8625    0\n",
       "8630    0\n",
       "8633    0\n",
       "8634    0\n",
       "8635    0\n",
       "8636    0\n",
       "8639    0\n",
       "8641    0\n",
       "8642    0\n",
       "8643    0\n",
       "8645    0\n",
       "8646    0\n",
       "8649    0\n",
       "8651    0\n",
       "8654    0\n",
       "8656    0\n",
       "8657    0\n",
       "8661    0\n",
       "8662    0\n",
       "8663    0\n",
       "8667    0\n",
       "Name: sentiment, Length: 1998, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate csv file\n",
    "# final_sliced_data.to_csv(\"text_data.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sliced_data.to_csv('pos_neg_score.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data.loc[cleaned_data['sentiment'] == 0].__len__())\n",
    "print(cleaned_data.loc[cleaned_data['sentiment'] == 1].__len__())\n",
    "#print(cleaned_data.loc[cleaned_data['sentiment'] == 2].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in word_tokenize(sent)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "class WordWeightAssign:\n",
    "    \"\"\"\n",
    "        This is the class to assign the word and its weight\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def collect_data(self):\n",
    "        self.unique_words = []\n",
    "        for element in self.data:\n",
    "            for item in element:\n",
    "                item = stemmer.stem(item)\n",
    "                self.unique_words.append(item)\n",
    "        self.unique_words = list(set(self.unique_words))\n",
    "        return self.unique_words\n",
    "    \n",
    "    def score_words(self):\n",
    "        review_word_scores = []\n",
    "        #import pdb;pdb.set_trace()\n",
    "        for item in self.data:\n",
    "            local_scores = np.zeros((4641,), dtype=np.int)\n",
    "            for element in item:\n",
    "                element = stemmer.stem(element)\n",
    "                current_index = self.unique_words.index(element)\n",
    "                if current_index:\n",
    "                    local_scores[current_index] += 1\n",
    "            review_word_scores.append(local_scores)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        return review_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4641\n",
      "1998\n"
     ]
    }
   ],
   "source": [
    "w = WordWeightAssign(cleaned_data['Description'].str.split(' ').tolist())\n",
    "print(w.collect_data().__len__())\n",
    "print(w.score_words().__len__())\n",
    "#print(w.score_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Subject</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a brilliant concept. Please start for ...</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brilliant concept :)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant stuff!! Waiting to see more..</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This idea will be considered as one of the top...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brilliant idea!! Adorably splendid dresses at ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description         Subject  \\\n",
       "0  This is a brilliant concept. Please start for ...  Great product!   \n",
       "1                               Brilliant concept :)             NaN   \n",
       "2            Brilliant stuff!! Waiting to see more..             NaN   \n",
       "3  This idea will be considered as one of the top...       Excellent   \n",
       "5  Brilliant idea!! Adorably splendid dresses at ...             NaN   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "5          1  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.534\n",
      "0.79\n",
      "0.498\n",
      "0.742\n",
      "0.428\n",
      "0.85\n",
      "0.548192771084\n",
      "0.855421686747\n",
      "[[ 646.  353.]\n",
      " [ 642.  357.]] 0.502002002002 [[ 794.  205.]\n",
      " [ 176.  823.]] 0.809309309309\n"
     ]
    }
   ],
   "source": [
    "labels = cleaned_data[\"sentiment\"].values;\n",
    "\n",
    "#import pdb;pdb.set_trace()  \n",
    "kf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "\n",
    "totalNBV = 0\n",
    "\n",
    "totalMatNBV = np.zeros((2,2));\n",
    "\n",
    "\n",
    "totalNBV_tfidf = 0\n",
    "\n",
    "totalMatNBV_tfidf = np.zeros((2,2));\n",
    "\n",
    "# tr, te = kf.split(sliced_data['Review Text'], labels)\n",
    "# td = []\n",
    "# for i in train_index:\n",
    "#     td.append(sliced_data['Review Text'][i])\n",
    "#import pdb;pdb.set_trace()\n",
    "review_text = cleaned_data['Description'].tolist()\n",
    "\n",
    "# break words\n",
    "review_words = []\n",
    "\n",
    "# for item in review_text:\n",
    "#     review_words += item.split(\" \")\n",
    "counter = 0\n",
    "for train_index, test_index in kf.split(review_text, labels):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #print(train_index.__len__(), test_index.__len__())\n",
    "    X_train = [review_text[i] for i in train_index]\n",
    "    X_test = [review_text[i] for i in test_index]\n",
    "    \n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=False, stop_words='english')\n",
    "    train_corpus_tf_idf = vectorizer.fit_transform(X_train) \n",
    "    test_corpus_tf_idf = vectorizer.transform(X_test)\n",
    "    \n",
    "    splitted_x_train = [item.split(' ') for item in X_train]\n",
    "    w = WordWeightAssign(splitted_x_train)\n",
    "    w.collect_data()\n",
    "    w.score_words()\n",
    "    train_corpus_scores = w.score_words()\n",
    "    \n",
    "    splitted_x_test = [item.split(' ') for item in X_test]\n",
    "    w = WordWeightAssign(splitted_x_test)\n",
    "    w.collect_data()\n",
    "    w.score_words()\n",
    "    test_corpus_scores = w.score_words()\n",
    "    \n",
    "    \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(train_corpus_scores, y_train)\n",
    "    result1 = nb.predict(test_corpus_scores)\n",
    "    test = nb.score(test_corpus_scores, y_test)\n",
    "    print(test)\n",
    "    \n",
    "    nbtfidf = MultinomialNB()\n",
    "    nbtfidf.fit(train_corpus_tf_idf, y_train)\n",
    "    result2 = nbtfidf.predict(test_corpus_tf_idf)\n",
    "    test = nbtfidf.score(test_corpus_tf_idf, y_test)\n",
    "    print(test)\n",
    "    \n",
    "    totalMatNBV = totalMatNBV + confusion_matrix(y_test, result1)\n",
    "    totalNBV = totalNBV + sum(y_test==result1)\n",
    "    \n",
    "    totalMatNBV_tfidf = totalMatNBV_tfidf + confusion_matrix(y_test, result2)\n",
    "    totalNBV_tfidf = totalNBV_tfidf + sum(y_test==result2)\n",
    "    \n",
    "    counter += 1\n",
    "    prev = cleaned_data.iloc[test_index, :]\n",
    "    test = pd.DataFrame( { 'Review Text': X_test , 'Sentiment': result1})\n",
    "    test.to_csv( 'sentiment_prediction_custom_'+str(counter)+'.csv' , index = False )\n",
    "    \n",
    "    test = pd.DataFrame( { 'Review Text': X_test , 'Sentiment': result2})\n",
    "    test.to_csv( 'sentiment_prediction_tfidf_'+str(counter)+'.csv' , index = False )\n",
    "    \n",
    "    \n",
    "print(totalMatNBV, totalNBV/1998.0, totalMatNBV_tfidf, totalNBV_tfidf/1998.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sid.polarity_scores(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
